Web Mining в R
================

План
====

Вступ
-----

Основний інструмент роботи: **R**. Перевага використання саме програмування для отримання інформації з веб-сторінок у дешевизні та гнучкості інструменту. Ви зможете пристосувати його під будь-яку задачу, яка стосується отримання інформації з веб-сторінок.

В цьому курсі мова буде йти про способи отримання інформації з веб-сторінок. Загалом є два основних механізми:

-   робота з **Web API**
-   парсинг

Оскільки мова йде про отримання і подальше зберігання даних, то вступне заняття буде у великій мірі присвячено форматам даних в **R**.

Працювати ми будемо з такими форматами даних:

-   **vector**
-   **data\_frame**
-   **list**

Крім цього, розглянемо, як обробити в **R** файл у форматі **json**, оскільки це найпоширеніший формат віддачі інформації з **API**.

Модуль 1. Робота з Web API
--------------------------

**Зміст:** бібліотека [httr](!https://github.com/r-lib/httr) та методи **POST** і **GET**

**Практична користь:** як отримати стандартизовані дані з бази веб-сторінки.

### Як працювати з будь-яким API

**Зміст:** алгоритм написання функції для отримання інформації з будь-якого сайту, у якого є **API**.

**Практична користь:** виникає проблема отримання даних із сайту, у якого є **API**, але нема готового інструменту взаємодії з ним в **R**. Як самому за короткий строк це зробити?

**Складність:** потрібно буде зрозуміти, як писати прості функції в **R** і як керувати організацією даних в **R**.

### Web API Facebook

**Зміст:** робота з бібліотекою [Rfacebook](!https://github.com/pablobarbera/Rfacebook). Додатково розглянемо програму [Facepager](https://github.com/strohne/Facepager/releases), через яку можна отримувати таку інформацію без програмування.

**Практична користь:** найшвидший спосіб отримати великий масив даних інформації з публічних сторінок та груп **Facebook**. Однак він не діє для персональних сторінок.

**Складність:** середня. Більшість функцій для викачки вже написано, просто розглянемо, як можна їх адаптувати до різних задач.

### API Twitter & VK

**Зміст:** робота з бібліотеками [twitteR](!https://cran.r-project.org/web/packages/twitteR/twitteR.pdf) та [vkR](!https://github.com/Dementiy/vkR).

**Практична користь:** розглянемо швидке і стандартизоване отримання інформації зі сторінок **VK** i **Twitter**.

**Складність:** середня. Більшість функцій для викачки вже написано, просто розглянемо, як можна їх адаптувати до різних задач.

Модуль 2. Парсинг
-----------------

**Зміст:** написання функцій для збирання і стандартизації контенту з коду веб-сторінок.

**Практична користь:** коли в сайту нема **API** або **API** віддає замало інформації іншого способу немає.

### Імітація роботи браузера в RSelenium

**Зміст:** можливості імітації роботи користувача в браузері через створення **Selenium Driver** в пакеті [RSelenium](!http://ropensci.github.io/RSelenium/).

**Практична користь:** найкращий спосіб отримувати дані з веб-сторінок, які вимагають взаємодії з користувачем (авторизація, навігація сторінками).

**Складність:** функції для роботии вже готові, однак потрібно вміти їх адаптувати та розбиратися в структурі коду веб-сайтів.

### Швидка обробка веб-сторінок у бібліотеці rvest

**Зміст:** бібліотека [rvest](!https://github.com/hadley/rvest) і філософія набору пакетів [tidyverse](!http://www.tidyverse.org/packages/). Економний режим парсингу веб-сторінок.

**Практична користь:** ці бібліотеки дозволяють парсити веб-сторінки з мінімальним навантаженням на оперативну пам'ять та процесор. Дуже корисно при великих обсягах викачування.

**Складність:** найбільша. Доведеться розібратися з базовими фукціями **tidyverse**, однак їх призначення інтуїтивно зрозуміле. Таке ознайомлення буде корисне не тільки для задач викачки даних, а і для роботи аналітика в **R** в принципі.

### Парсинг новин в readability

**Зміст:** інтеграція можливостей **Python** в **R** на прикладі швидкого парсингу тексту новин у пакеті [readability](!https://github.com/buriy/python-readability).

**Практична користь:** дуже простий спосіб отримати текст веб-сторінки, який не вимагає вивчення її структури.

**Складність:** найменша. Просто напишем просту функцію, в якому буде достатньо просто вставити посилання, щоб отримати повний текст статті за лінком.

Що далі?
--------

На завершення трохи про обробку тексту в пакетах [tm](!https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf), [tidytext](!tidytextmining.com) та [text2vec](!http://text2vec.org/). Це доцільно розглянути, оскільки велика частина інформації з вебу, яку ми отримуємо є текстовою. Ці бібліотеки є найкориснішими для **text mining**.

Пропонований пакет послуг занять:
=================================

1.  розповідь+презентація.
2.  практична робота (на прикладі конкретного ресурсу поглянемо, як працюють згадані вище інструменти) +завдання (відтворення пройденого на іншій веб-сторінці).
3.  код за змістом занять, за опомогою якого можна відтворити показане на заняттях.
4.  шпаршалки по базових поняттях і функціях роботи в R, форматах даних.

Установки
=========

Необхідні установлені технології:

-   R
-   RStudio
-   Java 8
-   Python
-   Selenium

Необхідні бібліотеки:

-   tidyverse
-   httr
-   RCurl
-   rvest
-   RSelenium
-   xml2
-   reticulate
-   rfacebook
-   twitteR
-   vkR
